%% Generate data part
clear all
clc
close all
t_max = 60;
x0 = abs(randn(1,1))*100; % = 1
[xt,yt] = generateData(t_max, x0); % dim(xt) = 61x1, dim(yt) = 60x1


%% Formulate prior
% We set the prior to a normal distribution, generated by taking the mu and
% sigma from 1000 datapoints. 
clc
number_of_samples = 1000;
x0 = abs(randn(1,1))*100; % = 1
[xt_prior,yt_prior] = generateData(number_of_samples,x0); % dim(xt) = 1x(t_max+1)

pd = fitdist(xt_prior,'Normal');
prior_mu = pd.mu;
prior_sigma = pd.sigma;

%% UPF Algorithm
% Repeat the experiment 100 times.

% SETTINGS
clc
% Set the number of time steps
T = 60;
% Set the number of particles
N = 10; 
%N = 200;
% Allocate space for saving parameter values
particles = zeros(T+1,N);
Ps = zeros(T+1,N);
predicted_particles = zeros(T+1,N);
predicted_Ps = zeros(T+1,N);

estimated_ys = zeros(T,N);
estimated_x_means = zeros(T+1,N);
importance_weights = ones(T,N)./N;%zeros(T,N);
% =================================================================

% INITIALIZATION, t = 0.
% Draw particles from the prior
particles(1,:) = normrnd(prior_mu, prior_sigma, 1, N); % 1xN
% Initiate noise variables
v0 = zeros(1,N);
n0 = zeros(1,N);
% Compute the particle mean
x_mean_0 = prior_mu; % mean(particles(1,:)); % 1x1
estimated_x_means(1,:) = x_mean_0;
% Copute Covariance matrix.
diffs = particles(1,:) - x_mean_0; % 1xN
P0 = diffs*diffs'; % 1x1
Ps(1,:) = P0; 
% Redefine the state rand var as a concatenation of the original state and
% noise variables.
[x_a, x_mean_a, P0_a] = getCorrespondingAugmentedVariables(particles(1,:), v0, n0 ); % dim(x_a) = 3xN    dim(estimated_x_a) = 3x1    dim(P0_a) = 3x3

% THE BIG MACHINERY
% Define 
previous_estimated_x_a = x_mean_a;
previous_P_a = P0_a;

alpha = 1;
beta = 0; % Comment from the paper: beta = 2 for suitable for Gaussian prior. Change to this?
kappa = 2; % Comment from the paper: kappa = 0 is a good default choise. Change to this?

% TODO: Adapt the content in the nestled loop to take the previous values
%       in each of the t:th steps. Not only using the original values. DONE?

for t = 1:T
    % a) Importance sampling step, using SUT.
    
    % % IS THIS PART NEEDED?? % % % % % % % % % % % % % %
    if t > 1
        vt = ones(1,N)*gamrnd(3,2); % zeros(1,N)
        nt = ones(1,N)*normrnd(0,0.00001); % zeros(1,N)
        [x_a, x_mean_a, P0_a] = getCorrespondingAugmentedVariables(particles(t,:), vt, nt );
    end   
    
    % % % % % % % % % % % % % % % % % % % % % % % % % % % 
    
    for i = 1:N
        % - Update the particles with the UKF:
        % * Calculate sigma points and their weights
        n_x = size(x_a,1);          % THESE PARAMETER definitions, should maybe be moved outside big loop?
        lambda = alpha^2 * (n_x + kappa) - n_x; 
        sqrt_matrix = sqrt((n_x+lambda)*previous_P_a); %3x3
        % Initialization
        previous_sigma_points = zeros(n_x,(2*n_x+1)); %3x7 
        previous_sigma_weights = zeros(2,(2*n_x+1)); %2x7 
        % The calculations
        previous_sigma_points(:,1) = previous_estimated_x_a;
        for sigma_point_i = 2:(n_x+1)
            previous_sigma_points(:,sigma_point_i) = previous_estimated_x_a + sqrt_matrix(:,sigma_point_i-1);
        end
        for sigma_point_i = (n_x+2):(2*n_x+1)
            previous_sigma_points(:,sigma_point_i) = previous_estimated_x_a - sqrt_matrix(:,sigma_point_i-4);
        end
        
        W0_m = lambda/(n_x+lambda);
        W0_c = lambda/(n_x+lambda) + (1 - alpha^2 + beta);
        previous_sigma_weights(1,1) = W0_m; 
        previous_sigma_weights(2,1) = W0_c; 
        previous_sigma_weights(1,2:(2*n_x+1)) = 1/(2*(n_x+lambda));
        previous_sigma_weights(2,2:(2*n_x+1)) = 1/(2*(n_x+lambda));
        
        % * Propagate particle into future (time update) 
        % Initialization
        current_sigma_points = zeros(n_x,(2*n_x+1)); %3x7 
        current_sigma_weights = zeros(2,(2*n_x+1)); %2x7 

        % The updates
        previous_sigma_x = previous_sigma_points(1,:);
        previous_sigma_v = previous_sigma_points(2,:);
        previous_sigma_n = previous_sigma_points(3,:);
        previous_t = t-1;

        current_sigma_points(1,:) = processModel(previous_sigma_x, previous_sigma_v, previous_t); % 1x7
        current_estimated_x = sum(previous_sigma_weights(1,:) .* current_sigma_points(1,:)); % 1x1
        current_P = sum(previous_sigma_weights(2,:) .* ((current_sigma_points(1,:)-current_estimated_x)*(current_sigma_points(1,:)-current_estimated_x)') ); %1x1
        current_sigma_point_propagations = observationModel(current_sigma_points(1,:),previous_sigma_n, t);% 1x7
        current_estimated_y = sum(previous_sigma_weights(1,:) .* current_sigma_point_propagations); % 1x1
        
        % * Incorporate new observation (measurement update)
        current_P_yy = sum(previous_sigma_weights(2,:) .* (current_sigma_point_propagations - current_estimated_y)*(current_sigma_point_propagations - current_estimated_y)' ); %1x1
        current_P_xy = sum(previous_sigma_weights(2,:) .* (current_sigma_points(1,:) - current_estimated_x)*(current_sigma_point_propagations - current_estimated_y)'); %1x1
        current_Kalman_gain = current_P_xy * inv(current_P_yy); %1x1
        current_estimated_x = current_estimated_x + current_Kalman_gain*(yt(t)-current_estimated_y);% 1x1    % CHANGE NAME on this variable?! 
        current_P = current_P - current_Kalman_gain*current_P_yy*current_Kalman_gain';% 1x1                  % CHANGE NAME on this variable?! 
        
        % - Sample x^
        new_particle = normrnd(current_estimated_x, current_P); % the (t+1) index is due to matlabs indexing, everywhere you see it.
        % - Set  
        predicted_particles(t+1,i) = new_particle;
        estimated_x_means(t+1,i) = current_estimated_x; %%%%% CORRECT?
        % estimated_ys(t,i) = 
        predicted_Ps(t+1,i) = current_P;
    end  

    % TODO: Figure out it some how incorporate the noise distributions here
    % somewhere? Or if it they always are zero. And the distributions are
    % used only for generating data. I mean, v0 and n0 here below, really?? 
    
    % Evaluate the importance weights up to a normalizing constant.
    estimated_ys(t,:) = observationModel(predicted_particles((t+1),:), n0, t); %%% RIGHT??
    %format long
    for i=1:N
        % the observation model is a Gaussian distribution
        
        obs_var = 1e-5;
        %(1/sqrt(2*pi*obs_var))
        %exp(-0.5*inv(obs_var))
        %exp(-0.5*(1/obs_var)*(yt(t)-estimated_ys(t,i))^2)
        likelihood = (1/sqrt(2*pi*obs_var)) * exp(-0.5*(1/obs_var)*(yt(t)-estimated_ys(t,i))^2)+1e-99; %Weird values!!!!
        % TODO: estimated_ys(t,i) is something else?
        % the process model is a Gamma distribution
        k = 3;
        theta = 2;
        %x_term = particles(t+1,i)-particles(t,i) % Is this really correct? Feels weird, but according to the notation it should be like this...
        x_term = predicted_particles(t+1,i) - particles(t,i); % (t+1) is referring to this current time step here! 
        prior = (x_term^(k-1)*exp(-x_term/theta))/((theta^k)*gamma(k)); % Weird values!!!!
        % the proposal distribution
        proposal = (1/sqrt(2*pi*predicted_Ps(t+1,i))) * ...
            exp(-0.5*(1/predicted_Ps(t+1,i))*(predicted_particles(t+1,i)-estimated_x_means(t+1,i))^2); % Weird values!!!!
        % the importance weight
        importance_weights(t,i) = likelihood*prior/proposal;
    end    
    % Normalize the importance weights.    
    importance_weights(t,:) = importance_weights(t,:) ./ ...
        sum(importance_weights(t,:));
    % ---------------------------------------
    % b) Selection step, resampling with residual resampling

    % Firstly, set this:
    w_res = N .* importance_weights(t,:);
    N_tilde = floor(w_res); % 1xN N_tilde = N_children
    % Secondly, perform a SIR procedure
    N_res = N-sum(N_tilde); %1x1
    %The modified weights
    w_res = (w_res-N_tilde)/N_res; %1xN
    % Draw the deterministic part
    i=1;
    for j=1:N
        %j
        %N_tilde(j)
        for k=1:N_tilde(j)
            %k
            new_indx(i) = j;
            i = i +1;
        end    
    end    
    
    % Draw the stocastic part
    cumDist = cumsum(w_res); 
    % All weights should sum up to 1, so cumDist(N) = 1
    while i<=N
        sample = rand(1,1); % (0,1]
        j = 1;
        while cumDist(j) < sample
            j = j+1;
        end
        new_indx(i) = j;
        i = i+1;
    end   

    % Keep particles according to the resampled indices.
    % Not sure this is the last thing, the output.
    particles((t+1),:) = predicted_particles((t+1),new_indx); 
    Ps((t+1),:) = predicted_Ps((t+1),new_indx); 

end    
%%
clc
size(importance_weights)
true_xs = xt % 61x1
number_of_states = size(true_xs,1);

% Is the estimated_x value at t the mean value of the 200 particle-values for
% that specific t?
estimated_xs = zeros(size(xt));
for t=1:T
    estimated_xs(t) = sum(importance_weights(t,:).*particles(t+1,:)); % TODO: Verify that it is coherent with 4.4.2 in the paper. 
end    

estimated_xs

% Mean square estimate!
MSE = (1/number_of_states)*sum((estimated_xs-true_xs).^2)
